---
title: "ML Final Project"
author: "Amanda, Tony, Harry, Geena"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r echo=F}
library(glmnet)
library(tidyverse)
library(tree)
library(ISLR2)
library(randomForest)
library(MLmetrics)
library(BART)
library(dplyr)
library(ggplot2)

shelter_clean <- read_csv("shelter_clean.csv")

shelter_clean$AnimalType <- as.factor(shelter_clean$AnimalType)
shelter_clean$Sex <- as.factor(shelter_clean$Sex)
shelter_clean$IntakeCondition <- as.factor(shelter_clean$IntakeCondition)
shelter_clean$OutcomeType <- as.factor(shelter_clean$OutcomeType)


set.seed(123)
train_index <- sample(1:nrow(shelter_clean), floor(0.8 * nrow(shelter_clean)))
train <- shelter_clean[train_index, ]
test <- shelter_clean[-train_index, ]
```

# Introduction

Describe the questions and data that we are interested in. (**TONY Can
you fill this out a bit plz, maybe just copy over some of the stuff from
the slides tbh**)

Data Cleaning Process to create shelter_clean.csv:

```{r eval=F}
#Load in datasets and initial cleaning
outcomes <- read_csv("Austin_Animal_Center_Outcomes_20250203.csv")
intakes <- read_csv("Austin_Animal_Center_Intakes_20250203.csv")

#
outcomes_clean <- outcomes |> mutate(OutcomeDate = as.Date(DateTime, '%m/%d/%Y %H:%M:%S')) |>
  select(-c(DateTime, MonthYear, `Outcome Subtype`, Name, `Animal Type`, Breed, Color))
intakes_clean <- intakes |>  mutate(IntakeDate = as.Date(DateTime, '%m/%d/%Y %H:%M:%S')) |> select(-c(DateTime, MonthYear))


#inner join the datasets
data <- merge(intakes_clean, outcomes_clean, by="Animal ID")

#Clean up merged dataset, and remove the duplicate rows from the multiple stays
clean <- data |>
  group_by(`Animal ID`) |>
  mutate(LengthofStay = OutcomeDate - IntakeDate) |>
  group_by(`Animal ID`) |>
  mutate(drop = LengthofStay < 0 |(LengthofStay >= max(LengthofStay) & n() > 1)) |>
  ungroup() |>
  filter(drop == F) |>
  select(-c("drop"))

#Create Date of Birth and then cast Age as a numeric variable
shelter_clean <- clean |>
  mutate(DOB = as.Date(`Date of Birth`, '%m/%d/%Y'),
         LengthofStay = as.numeric(LengthofStay),
         Age = case_when(
           str_detect(`Age upon Outcome`, "years") ~ as.numeric(gsub(" years", "", `Age upon Outcome`)),
           str_detect(`Age upon Outcome`, "months") ~ as.numeric(gsub(" months", "", `Age upon Outcome`))/12,
           str_detect(`Age upon Outcome`, "year") ~ as.numeric(1),
           str_detect(`Age upon Outcome`, "month") ~ as.numeric(1/12),
           str_detect(`Age upon Outcome`, "weeks") ~ as.numeric(gsub(" weeks", "", `Age upon Outcome`))/52,
           str_detect(`Age upon Outcome`, "week") ~ 1/52,
           str_detect(`Age upon Outcome`, "days") ~ as.numeric(gsub(" days", "", `Age upon Outcome`))/365,
           str_detect(`Age upon Outcome`, "day") ~ 1/365,
         )) |>
  select(`Animal ID`, Name, IntakeType = `Intake Type`, IntakeCondition = `Intake Condition`, AnimalType=`Animal Type`, Sex = `Sex upon Intake`, Age, OutcomeType=`Outcome Type`, OutcomeDate, LengthofStay, DOB)

#Make all of the categorical variables as factors
shelter_clean$AnimalType <- as.factor(shelter_clean$AnimalType)
shelter_clean$Sex <- as.factor(shelter_clean$Sex)
shelter_clean$IntakeCondition <- as.factor(case_when(
  shelter_clean$IntakeCondition %in% c("Sick", "Agonal", "Injured", "Med Attn", "Med Urgent", "Medical", "Neurologic", "Parvo") ~ "Medical",
  shelter_clean$IntakeCondition %in% c("Other", "Unknown") ~ "Other",
  TRUE ~ as.character(shelter_clean$IntakeCondition)
))

#Create transformed numeric variables
shelter_clean <- shelter_clean |> mutate(log_LOS = log(1+LengthofStay), log_Age = log(1+Age, 10))

#Create the logistic regression adoption variable for outcome type
shelter_clean <- shelter_clean |>
  mutate(Adoption = ifelse(OutcomeType == "Adoption", 1, 0), age2 = sqrt(Age)) |> 
  filter(!is.na(Adoption))
```

# Methods

## Q1: Using the available predictors, is it possible to predict the length of stay of an animal that comes into the Austin animal shelter?

Initially, we tried to fit a multiple linear regression to predict the
length of stay of an animal, which gave us the following results:

```{r}
lin_model <- lm(as.numeric(log_LOS) ~ log_Age + AnimalType + IntakeCondition + Sex + OutcomeType,data=shelter_clean)
summary(lin_model)
```

This model was not the best at predicting the length of stay of animals.
The adjusted R-squared value was only about .386. If we look at the
residual standard error though, we can see that our average error is
only about 1 day, which is pretty good in context. However, we should
also consider how many of the data points were from short term stays
rather than longer term stereotypical adoption cases when interpreting
the error of 1 day.

From the output, we can see that there isn't a very strong linear
relationship between the predictors and the response variable. However,
before we move on to another model to predict the length of stay, we can
try to regularize the model using lasso regression to see if we can
suppress some of the less important predictors - which will be helpful
especially as almost all of the predictors are indicator variables which
are hard to control in regular linear regression.

```{r}
#Lasso Regularization
set.seed(123)
response <- train$log_LOS
predictors <- model.matrix(log_LOS~log_Age + AnimalType + IntakeCondition + Sex + OutcomeType, shelter_clean)

ridge_model <- cv.glmnet(predictors[train_index, ], response, alpha = 1)
plot(ridge_model)
ridge_model$lambda.min

predict(glmnet(predictors[train_index, ], response, alpha = 1), type = "coefficients", s = ridge_model$lambda.min)[1:26,]



pred <- predict(glmnet(predictors[train_index, ], response, alpha = 1), s = ridge_model$lambda.min, newx = predictors[-train_index, ])
sqrt(mean((pred - test$log_LOS)^2))
```

The Intake Condition variable is not as important now as shown by the
smaller (sometimes 0) new coefficients, while most of the other
coefficients were mostly unaffected by the lasso regression.
Additionally from the output, we can see that the lambda chosen by the
cross validation was very small, indicating that only a minimal
regularization effect was applied to the linear regression (as larger
lambda values tend to have a larger effect). This is reasonable, as our
data set was very large (over 10,000 observations with only a handful of
predictor variables), and these regularization techniques generally work
best when the number of predictors and number of data points are of
similar magnitude to each other.

The linear regression and lasso regularization helped predict length of
stay and see which variables were important, but it does assume that
there are linear relationships between the predictor and response
variable. To allow for more flexible relationships, we can fit a
Generalized Additive Model (GAM) to model the non-linear effects of
different predictors.

***TONY YOU CAN DO THE GAM MODEL THING HERE THX***

Finally, we can switch to a completely new method, and see if a
regression tree can offer any additional information about which
predictors are more important, and predict the length of stay
accurately.

```{r}
#Try simple regression tree
tree <- tree(log_LOS~log_Age + AnimalType + IntakeCondition + Sex + OutcomeType, train)
summary(tree)
plot(tree)
text(tree, pretty=1)
yhat <- predict(tree, newdata = test)
RMSE(yhat, test$log_LOS)
```

From the output, we have a pretty clean, simple tree. Since there are
only 6 terminal nodes, we probably don't need to prune this tree, but we
can check to be sure.

```{r}
#Prune?
set.seed(123)
cv_tree <- cv.tree(tree,  K = 10) #10-fold cross validation
plot(cv_tree$size, cv_tree$dev,  type = 'b', xlab = 'Tree size', ylab = 'Cross validation error')

pred <- predict(tree, newdata = test)
RMSE(pred, test$log_LOS)^2
```

According to the graph, our original tree with 6 terminal nodes gives
the best (lowest) error. Using this tree, we can see that our test error
is 1.09, which is similar to our test errors from earlier methods. Maybe
trying a random forest to help with potential variance in the model.

```{r}
#Random Forest
set.seed(123)
sequence_predictors <- seq(1, 5, by = 1)
# We create the number of predictors in {1, 2, ..., 5}
forest_test_error <- rep(0, length(sequence_predictors))
# We create a sequence of test errors with 0 values for each number of trees
for (i in 1:length(sequence_predictors))
{
  my_forest <- randomForest(log_LOS~log_Age + AnimalType + IntakeCondition + Sex + OutcomeType, data = train, mtry = sequence_predictors[i], ntree = 100, importance = TRUE)
  pred <- predict(my_forest, newdata = test)
  forest_test_error[i] <- RMSE(pred, test$log_LOS)^2
}

### Plot the test errors
forest_test_error_data_frame <- as.data.frame(forest_test_error)
ggplot(data = forest_test_error_data_frame, aes(x = seq(1,length(sequence_predictors), by = 1), y = forest_test_error)) + geom_line() + xlab("The number of predictors") + ylab("Test error")
min <- which.min(forest_test_error)
min

my_forest <- randomForest(log_LOS~log_Age + AnimalType + IntakeCondition + Sex + OutcomeType, data=train, mtry = min, importance=TRUE)

pred <- predict(my_forest, newdata = test)
RMSE(pred, test$log_LOS)^2

importance(my_forest)
varImpPlot(my_forest)
```

Using cross validation, we chose the number of predictors to choose from
at each split to be 2. After creating our random forest, we can see the
RMSE of the random forest predictions, 0.935, is much better than all of
the other models. Additionally, using this method, we can see which
predictors are most important for this model - namely Outcome Type is
important using both metrics, while the two metrics disagree over the
importance of the other variables. In terms of increasing the MSE,
Intake condition and Animal Type were important, while to increase the
node purity, the Age and Sex were more important.

Overall, we evaluated several models to predict the length of stay of an
animal. Our models prove to be fairly good at predicting length of stay.
There was an average error (RMSE) of only 1 or so in the test sets for
most of our models, which means we are off by an average of 1 day when
predicting which is pretty good in context. However, we must also
consider that many length of stays are fairly short. Random forests
seemed to provide the most accurate predictions, and Outcome Type was
consistently the most important predictor across models, while Age and
Animal Type were pretty important, and Lasso regression decided Intake
Condition was not very important to predicting our response variable.

## Q2: Can we predict adoptability based on the age upon intake?

```{r}
library(plotROC)
library(ggplot2)
mean(shelter_clean$Adoption)
```

About 61% animals were adopted in the dataset.

```{r}
ggplot(data = shelter_clean) + geom_histogram(aes(x = Age), fill="lavender",color="darkgray")+labs(title="Animals at ACC based on age upon intake")
```

The data is skewed heavily right; most of the animals are young.

```{r}
# Represent the relationship with a model
shelter_clean |>
  ggplot(aes(x = Age, y = Adoption)) +
  # Consider a logistic regression model
  geom_smooth(method = "glm", se = FALSE, 
              method.args = list(family = "binomial"),
              color = "steelblue", size = 2) + 
  # Show original data
  geom_point(size = 4, alpha = 0.5) +
  labs(x = "Age upon intake", 
       y = "Adoption outcome",
       title = "Logistic regression model to predict adoption based on age upon intake")
```

Logistic regression is compatible with our research question here
because of the categorical nature of our question. When trying to
predict whether or not an animal will or will not be adopted, logistic
regression serves as a helpful model.

```{r}
# Fit the model
fit_log <- glm(Adoption ~ Age, data = shelter_clean, family = "binomial")

# Take a look at the model summary
summary(fit_log)
```

The output gives the logit-form of the model which is: ln(p hat / 1-p
hat) = 0.62524 - 0.11009 \* Age, where p hat is the probability of the
animal being adopted (1 = adopted, 0 = not adopted). This also shows
that age is a significant predictor of adoption because of the
significantly low Pr(\>\|z\|) values.

```{r}
library(dplyr)
shelter_clean |> 
  # Use the expression of the model
  mutate(probability = exp(0.62524 - 0.11009 * Age)/(1 + exp(0.62524 - 0.11009 * Age))) |>
  select(Age, Adoption, probability)
```

```{r}
shelter_predict <- shelter_clean |>
  # Create new variables for probability and predicted values
  mutate(probability = predict(fit_log, type = "response"),
         predicted = ifelse(probability > 0.3, 1, 0)) |>
  select(Age, Adoption, probability, predicted)

# Take a look
head(shelter_predict, 20)
```

```{r}
# Represent the relationship with a model
shelter_predict |>
  ggplot(aes(x = Age, y = Adoption)) +
  # Consider a logistic regression model
  geom_smooth(method = "glm", se = FALSE, 
              method.args = list(family = "binomial"),
              color = "steelblue", size = 2) + 
  # Show original data, colored by predicted values
  geom_point(size = 4, alpha = 0.5, 
             aes(color = as.factor(predicted))) + # add as.factor() to only see 2 categories
  labs(x = "Age", 
       y = "Outcome ",
       title = "Logistic regression model to predict adoption based on age")
```

The blue points are predicted to have been adopted and the red ones are
predicted to have been not adopted. Therefore, the top left corner of
blue points shows the true positive points (predicted as adopted,
actually adopted), while the bottom right red points show the true
negative points (predicted as not adopted, actually not adopted).

```{r}
ROC <- shelter_predict |>
  ggplot() + 
  # the predictions are based on the probability values
  geom_roc(aes(d = Adoption, m = probability), n.cuts = 10)
ROC
calc_auc(ROC)$AUC
```

The area under curve (AUC) value of 0.512 demonstrates that the logistic
regression model is not good at predicting. It is almost as good as
random chance (which means it is bad at predicting).

## Q3: Can we predict adoption based on intake type?

```{r}
stray_abandoned <- data %>% filter(Intake.Type %in% c("Stray", "Abandoned"))
public_assist_owner_surrender <- data %>% filter(Intake.Type %in% c("Public Assist", "Owner Surrender"))


total_stray_abandoned <- nrow(stray_abandoned)
total_public_assist_owner_surrender <- nrow(public_assist_owner_surrender)


adopted_stray_abandoned <- sum(stray_abandoned$Outcome.Type == "Adoption")
adopted_public_assist_owner_surrender <- sum(public_assist_owner_surrender$Outcome.Type == "Adoption")


adopted_stray_abandoned_pct <- (adopted_stray_abandoned / total_stray_abandoned) * 100
adopted_public_assist_owner_surrender_pct <- (adopted_public_assist_owner_surrender / total_public_assist_owner_surrender) * 100


cat("Percentage of Stray or Abandoned intakes that were adopted:", adopted_stray_abandoned_pct, "%\n")
cat("Percentage of Public Assist or Owner Surrender intakes that were adopted:", adopted_public_assist_owner_surrender_pct, "%\n")

data_merged <- merge(intakes, outcomes, by = "Animal.ID")

data_merged <- data_merged %>%
  mutate(Intake.Category = case_when(
    Intake.Type %in% c("Stray", "Abandoned") ~ "Stray/Abandoned",
    Intake.Type %in% c("Public Assist", "Owner Surrender") ~ "Public Assist/Owner Surrender",
    TRUE ~ "Other"
  ))

data_filtered <- data_merged %>%
  filter(Intake.Category != "Other")

#data
data_filtered <- data_filtered %>%
  mutate(Adoption.Status = ifelse(Outcome.Type == "Adoption", "Adopted", "Not Adopted"))
adoption_counts <- data_filtered %>%
  group_by(Intake.Category, Adoption.Status) %>%
  summarise(Count = n(), .groups = "drop")

#bar chart
ggplot(adoption_counts, aes(x = Intake.Category, y = Count, fill = Adoption.Status)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Adoption vs Not Adoption by Intake Category",
       x = "Intake Category",
       y = "Number of Animals") +
  theme_minimal() +
  scale_fill_manual(values = c("Adopted" = "green", "Not Adopted" = "red"))
```

Our initial hypothesis for this question was that because animals that
are in a shelter because of public assist or owner surrender have more
human experience, and thus are more likely to be adopted compared to
strays. From initial analysis we see that there seem to be
proportionally more animals adopted that are in the shelter because of
public assistance or owner surrender that animals that are stray. This
does not confirm our hypothesis, however. To further confirm this we
calculate the adoption rates of public assist/owner surrender and
stray/abandoned animals and found that public assist/owner surrendered
animals had a 70.60% adoption rate as compared to the former which had a
62.45% adoption rate. While the adoption rate is greater for the animals
with prior human interaction, is this difference significant?

```{r}
#SIGNIFICANCE USING PROPORTION TEST
stray_abandoned <- data_filtered %>% filter(Intake.Category == "Stray/Abandoned")
surrender <- data_filtered %>% filter(Intake.Category == "Public Assist/Owner Surrender")

#Adoption counts
adopted_stray <- sum(stray_abandoned$Adoption.Status == "Adopted")
not_adopted_stray <- sum(stray_abandoned$Adoption.Status != "Adopted")

adopted_surrender <- sum(surrender$Adoption.Status == "Adopted")
not_adopted_surrender <- sum(surrender$Adoption.Status != "Adopted")

#Proportion test
prop.test(
  x = c(adopted_stray, adopted_surrender),      # number of adoptions
  n = c(nrow(stray_abandoned), nrow(surrender)),# total in each group
  correct = FALSE                                # disables Yates' correction
)
```

By using a proportion test we can test the significance of this
difference. The resulting p-value \< 2.2e-16 confirms that this
difference in adoption rates is in fact significant at a 0.05 confidence
interval. Now that we know that the intake type is a significant
predictor for an animals adoption, we can begin fitting models with this
predictor.

```{r}
#LOGISTIC REG MODEL
data_filtered$AdoptedBinary <- ifelse(data_filtered$Adoption.Status == "Adopted", 1, 0)

data_filtered$Intake.Category <- factor(data_filtered$Intake.Category)

model <- glm(AdoptedBinary ~ Intake.Category, data = data_filtered, family = "binomial")

summary(model)

library(pROC)
predicted_probs <- predict(model, type = "response")
true_labels <- data_filtered$AdoptedBinary
roc_obj <- roc(true_labels, predicted_probs)
plot(roc_obj, main = "ROC Curve for Adoption Prediction", col = "blue", lwd = 2)
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 3)))
```

We chose the logistic regression model because of its advantage of ease
of implementation as well as it being well-suited for binary
classification tasks. It turns out, however, that it was not very good
at predicting adoption based on intake type. Just from the ROC curve
alone we can see that the curve does not even come close to the top left
corner of the graph. To further confirm this we find that the AUC=0.536,
which signifies that the model is equivalent to random guessing.It
became apparent that using just intake type as a predictor isn't great
for fitting models. Thus we added additional predictors to get a better
model.

```{r}
library(dplyr)
library(pROC)

convert_age <- function(age_string) {
  age_string <- tolower(age_string)
  if (grepl("day", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 365
  } else if (grepl("week", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 52
  } else if (grepl("month", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 12
  } else if (grepl("year", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string))
  } else {
    NA
  }
}

data_model <- data_filtered %>%
  mutate(
    IntakeCategory = ifelse(Intake.Category == "Stray/Abandoned", "Stray", "Surrender"),
    IntakeCategory = as.factor(IntakeCategory),
    AgeYears = sapply(`Age.upon.Intake`, convert_age),
    IntakeCondition = as.factor(Intake.Condition),
    AnimalType = as.factor(Animal.Type.y),
    Sex = as.factor(Sex.upon.Intake),
    AdoptedBinary = as.factor(AdoptedBinary)
  ) %>%
  filter(!is.na(AgeYears) & !is.na(IntakeCondition) & !is.na(Sex))

set.seed(123)
n <- nrow(data_model)
train_idx <- sample(1:n, size = 0.8 * n)

train_data <- data_model[train_idx, ]
test_data <- data_model[-train_idx, ]

#logistic
model <- glm(AdoptedBinary ~ IntakeCategory + AgeYears + IntakeCondition + AnimalType + Sex,
             data = train_data, family = "binomial")

pred_probs <- predict(model, newdata = test_data, type = "response")
pred_labels <- ifelse(pred_probs > 0.5, "1", "0")
true_labels <- as.character(test_data$AdoptedBinary)

roc_obj <- roc(as.numeric(true_labels), pred_probs)
auc_val <- auc(roc_obj)

accuracy <- mean(pred_labels == true_labels)

summary(model)
cat("AUC:", round(auc_val, 3), "\n")
cat("Accuracy:", round(accuracy, 3), "\n")
```

Now that we have more predictors, we used 80% of the data as training
and 20% as testing to get a decent logistic regression model with an AUC
of 0.731 and an accuracy of 0.712. It also seems that the most
significant predictors are age and animal type. For the sake of finding
a better model, we will try fitting a KNN classification with multiple
predictors to see if it is better.

```{r}
library(class)  
library(pROC)    
library(dplyr)
convert_age <- function(age_string) {
  age_string <- tolower(age_string)
  if (grepl("day", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 365
  } else if (grepl("week", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 52
  } else if (grepl("month", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string)) / 12
  } else if (grepl("year", age_string)) {
    as.numeric(gsub("[^0-9]", "", age_string))
  } else {
    NA
  }
}

data_knn <- data_filtered %>%
  mutate(
    IntakeNum = ifelse(Intake.Category == "Stray/Abandoned", 1, 0),
    AgeNum = sapply(Age.upon.Intake, convert_age),
    IntakeConditionNum = as.numeric(factor(Intake.Condition)),
    AdoptedBinary = as.factor(AdoptedBinary)
  ) %>%
  filter(!is.na(AgeNum) & !is.na(Intake.Condition))  # Drop NAs

set.seed(123)
n <- nrow(data_knn)
train_idx <- sample(1:n, size = 0.8 * n)

train <- data_knn[train_idx, ]
test  <- data_knn[-train_idx, ]

# Features for KNN (scaled for fair distance calculation)
train_X <- scale(as.matrix(train[, c("IntakeNum", "AgeNum", "IntakeConditionNum")]))
test_X  <- scale(as.matrix(test[, c("IntakeNum", "AgeNum", "IntakeConditionNum")]),
                 center = attr(train_X, "scaled:center"),
                 scale = attr(train_X, "scaled:scale"))

train_y <- train$AdoptedBinary
test_y  <- test$AdoptedBinary
knn_pred <- knn(train = train_X, test = test_X, cl = train_y, k = 5, prob = TRUE)

knn_prob <- ifelse(knn_pred == "1", attr(knn_pred, "prob"), 1 - attr(knn_pred, "prob"))
test_y_num <- as.numeric(as.character(test_y))

#ROC and AUC
knn_roc <- roc(test_y_num, knn_prob)
plot(knn_roc, main = "ROC Curve for KNN with Intake Type + Age + Condition", col = "blue", lwd = 2)
auc_val <- auc(knn_roc)
print(paste("AUC:", round(auc_val, 3)))
```

Since we have classification variables we use KNN classification and
attempted to predict adoption based on intake type, intake condition,
and age. Already from the ROC curve we have a great model, since the
curve is closer to the top left of the graph. This is confirmed with the
AUC of 0.768, signifying a good model and marginally better than the
logistic regression model previously fitted.

# Conclusion and Future Work

*Conclusion and Future work instructions (**DELETE**): Summarize the
results you obtain from the questions. You can also include some
potential future work, such as those that you are not able to analyze in
the final project or those that the current analyses are not
sufficiently convincing.*

Q1: Predicting Length of Stay

-   Most of the models showed pretty strong predictive accuracy (a
    little less with the Linear Regression with the smaller R^2^ value,
    but the RMSE of \~1 day was pretty good). Overall, Animal Type
    seemed to be the most important predictor in predicting length of
    stay.

Q2: Predicting adoption based on intake age

-   Age is a significant predictor for this model, with an AUC of 0.511.
    The logistic regression performance was pretty poor, which may be
    due to longer stays. In the future, we could probably do additional
    classification models such as KNN, or adding additional predictors.

Q3: Predicting adoption based on intake type

-   Intake type is a significant predictor, however the logistic
    regression was not good KNN classification approach was fairly
    decent (AUC = 0.768).

# Contributions

Contributions mirror our presentations.

**Amanda**: Data cleaning part of introduction, and the first half of
Question 1

**Harry**: All of Question 3

**Tony**: Introduction and second half of Question 1.

**Geena**: All of Question 2 (initial observations, logistic regression,
logistic regression visualization, ROC plot, AUC value)
